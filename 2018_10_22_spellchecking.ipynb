{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description and pre-requisites:\n",
    "## Should have a list of words at hand\n",
    "- Some public list of all words in a language\n",
    "    - Google NGrams - http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
    "    - Project Gutenberg https://www.gutenberg.org/catalog/\n",
    "- Words encountered in indexed documents (can also contain errors)\n",
    "    - Manually add domain specific words\n",
    "    \n",
    "## Finding the correct word - Edit Distance\n",
    "- Measure the number of single-character edits required to change one word into another\n",
    "- Edits are: insert, delete, replace\n",
    "- Solved with dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance Algorithm:<br>\n",
    "1. Initialize the matrix with zeros\n",
    "2. Initialize 1st row and column with numbers from 0 to N and  from 0 to M\n",
    "    * this is the cost of inserting a letter, when starting from the empty string\n",
    "3. The value of each of the remaining cells is: <br>\n",
    "<b>m[i, j] = min(<br>\n",
    "            m[i-1,j] + 1, # insert in s1\n",
    "            m[i,j-1] + 1, # insert in s2\n",
    "            m[i-1,j-1] + (0 if s1[i]==s2[j] else 1) # replace\n",
    ")</b> <br>\n",
    "4. The min edit distance is at m[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|-|-|S|A|T|U|R|D|A|Y|\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|-|0|1|2|3|4|5|6|7|8|\n",
    "|S|1|0|1|2|3|4|5|6|7|\n",
    "|U|2|1|1|2|2|3|4|5|6|\n",
    "|N|3|2|2|2|3|3|4|5|6|\n",
    "|D|4|3|3|3|3|4|3|4|5|\n",
    "|A|5|4|3|4|4|4|4|3|4|\n",
    "|Y|6|5|4|4|5|5|5|4|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.__ : Compute the edit distance between the words elephant and relevant. <br/>\n",
    "__Exercise 2.__ : Implement the minimum edit distance algorithm.\n",
    "\n",
    "Some corpora of misspellings can be found here: https://www.dcs.bbk.ac.uk/~ROGER/corpora.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4., 5., 6., 7., 8.],\n",
       "       [1., 0., 1., 2., 3., 4., 5., 6., 7.],\n",
       "       [2., 1., 1., 2., 3., 4., 5., 6., 7.],\n",
       "       [3., 2., 2., 1., 2., 3., 4., 5., 6.],\n",
       "       [4., 3., 3., 2., 1., 2., 3., 4., 5.],\n",
       "       [5., 4., 4., 3., 2., 2., 3., 4., 5.],\n",
       "       [6., 5., 5., 4., 3., 3., 3., 4., 5.],\n",
       "       [7., 6., 6., 5., 4., 4., 4., 4., 5.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Return the minumum edit distance between the two words\n",
    "    \"\"\"\n",
    "    s1_len = len(s1) + 1\n",
    "    s2_len = len(s2) + 1\n",
    "    \n",
    "    matrix = np.zeros((s1_len, s2_len))\n",
    "    \n",
    "    for i in range(s1_len):\n",
    "        matrix[i][0] = i\n",
    "    \n",
    "    for i in range(s2_len):\n",
    "        matrix[0][i] = i\n",
    "    \n",
    "    for row in range(1, s1_len):\n",
    "        for col in range(1, s2_len):\n",
    "            matrix[row][col] = np.amin(np.array([\n",
    "                matrix[row - 1][col] + 1,\n",
    "                matrix[row][col - 1] + 1,\n",
    "                matrix[row - 1][col - 1] + (0 if s1[row - 1] == s2[col - 1] else 1)\n",
    "            ]))\n",
    "\n",
    "    return matrix\n",
    "\n",
    "edit_distance('lullaby', 'lollipop') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: <br>\n",
    "edit_distance('lullaby', 'lollipop') <br>\n",
    "> 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants:\n",
    "- Weighted Edit Distance\n",
    "- Keeping a backtrack of the symbol alignments\n",
    "- If a backtrack is not needed we can use only two matrix rows (memory reduced from O(nm) to O(2m))\n",
    "\n",
    "## Some implementations:\n",
    "- Levenstein Edit distance algorithm for Python in Cython for high performance \n",
    "    - https://github.com/gfairchild/pyxDamerauLevenshtein\n",
    "- Peter Norvig's implementation\n",
    "    - Idea : generate all possible words at edit distance 1 and 2, select those that are most probable (using a language model and priority of edit distance)\n",
    "    - Can incorporate error model (some data with common errors http://aspell.net/test/)\n",
    "    - http://norvig.com/spell-correct.html\n",
    "- Library TextBlob - contains code for training a Spellchecker from text\n",
    "    - https://github.com/sloria/TextBlob/blob/14f22102251ce1f02e8bcb3e74f86c037e3df822/textblob/_text.py#L1322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"I havv goood speling!\")\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyxDamerauLevenshtein\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "print(damerau_levenshtein_distance('smtih', 'smith'))\n",
    "print(normalized_damerau_levenshtein_distance('smtih', 'smith'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Problem with the Edit Distance approach:\n",
    "- Do we need to compute the edit distance to all dictionary terms? - __slow__\n",
    "- Solution 1: \n",
    "    - build __charcter n-gram index__ - n-gram:word\n",
    "        - Example: av -> avocado-> pavement, etc.\n",
    "    - retrieve the postings of each n-gram in the query\n",
    "    - merge postings, getting the words with most n-gram overlap\n",
    "    - car can get corrected to carbon, then to normalize the metric for overlap we use __Jaccard coefficient__:\n",
    "        - $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ \n",
    "        - If more than a threshold, then the words match\n",
    "- Solution 2:\n",
    "    - having to correct the word lates we will possibly compute edit distance to the words late, latest, latte\n",
    "    - for each variant we will compute the edit distance of the common prefix late\n",
    "    - rather use __tries__: store the whole vocabulary in a large trie, where one node is a symbol and the branches are the possible symbols following it\n",
    "    - __compute the edit distance just once for the same prefix__!\n",
    "    - Reference: http://stevehanov.ca/blog/index.php?id=114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.__ Implement the n-gram overlap spell-checker. Find the best Jaccard coefficient for the chosen dataset. <br>\n",
    "__Exercise 4.__ If you already decided what will be your project, build and tune your own spellchecker (or even one for Bulgarian!):\n",
    "- Choose and implementation and a list of words (Textblob's spellchecker is pretrained on Gutenberg's data)\n",
    "- Run your corpus through the spell-checker and find word, which were corrected, but are spelled correctly\n",
    "- Add them to the corpus of words you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from string import punctuation\n",
    "from os import scandir\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def preprocess_documents(path):\n",
    "    \"\"\"\n",
    "    Returns a list of tokens for a document's content. \n",
    "    Tokens should not contain punctuation and should be lower-cased.\n",
    "    \"\"\"\n",
    "    tokenized_documents = []\n",
    "    for doc_f in scandir(path):\n",
    "        if not doc_f.is_file():\n",
    "            continue\n",
    "        content = open(doc_f).read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        tokens = []\n",
    "        for _sent in sentences:\n",
    "            sent_tokens = tokenizer.tokenize(_sent)\n",
    "            sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "            tokens += sent_tokens\n",
    "        tokenized_documents.append(tokens)\n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newsgroups',\n",
       " 'rec.autos',\n",
       " 'path',\n",
       " 'cantaloupe.srv.cs.cmu.edu',\n",
       " 'crabapple.srv.cs.cmu.edu',\n",
       " 'fs7.ece.cmu.edu',\n",
       " 'europa.eng.gtefsd.com',\n",
       " 'howland.reston.ans.net',\n",
       " 'usc',\n",
       " 'cheshire.oxy.edu']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = preprocess_documents('data/mini_newsgroups/rec.autos/')\n",
    "tokenized_documents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', 'sd', 'df']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def to_infixes(word):\n",
    "    word_len = len(word)\n",
    "    \n",
    "    if word_len < 2:\n",
    "        return []\n",
    "    if word_len == 2:\n",
    "        return [word]\n",
    "    \n",
    "    infixes = []\n",
    "    \n",
    "    for i in range(1, word_len):\n",
    "        infixes.append(word[i - 1] + word[i])\n",
    "        \n",
    "    return infixes\n",
    "\n",
    "def build_ngram_postings(n, tokenized_documents):\n",
    "    \"\"\"\n",
    "    Create postings from the tokenized documents - \n",
    "    For each n-gram we want to have the list of words, containing that n-gram. (E.g. av: [avocado, pavement, ...])\n",
    "    \"\"\"\n",
    "    # TODO: use n\n",
    "    \n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    bigrams = []\n",
    "    \n",
    "    for a in alphabet:\n",
    "        for b in alphabet:\n",
    "            bigrams.append(a + b)\n",
    "    \n",
    "    postings = {bigram: [] for bigram in bigrams}\n",
    "    \n",
    "\n",
    "    for bigram in bigrams:\n",
    "        for doc in tokenized_documents:\n",
    "            for word in doc:\n",
    "                for infix in to_infixes(word):\n",
    "                    if bigram == infix:\n",
    "                        postings[bigram].append(word)\n",
    "\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_postings = build_ngram_postings(2, tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'similar',\n",
       " 'richard',\n",
       " 'archive-name',\n",
       " 'part',\n",
       " 'cars',\n",
       " 'are',\n",
       " 'various',\n",
       " 'appearing',\n",
       " 'separate']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_postings['ar'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_spellcheck(word, postings, n, jaccard_threshold=0.7):\n",
    "    infixes = to_infix(word)\n",
    "    \n",
    "    \n",
    "    \"\"\"Return the word from the postings structure, which is closest to the input word.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example output: __<br>\n",
    "postings = build_ngram_postings(2, tokenized_documents) <br>\n",
    "print(\"Postings of bi-gram 'ar':\")<br>\n",
    "print(postings['ar'][:10])<br>\n",
    "print(\"Closest words to enginering:\")<br>\n",
    "print(ngram_spellcheck('enginering', postings, 2, jaccard_threshold=0.7))<br>\n",
    "\n",
    ">Postings of bi-gram 'ar':<br>\n",
    ">['article', 'darren', 'cars', 'hard', 'yard', 'compartment', 'are', 'early', 'area', 'antiauthoritarian']<br>\n",
    ">Closest words to enginering:<br>\n",
    ">['engineering', 'enginerring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Building a Grammarly-like spelling/grammar correction for Bulgarian can be a good course project. \n",
    "If anyone is interested, we can discuss how to collect data and how to approach the task._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(p1, p2):\n",
    "    return - p1 * math.log(p1, 2) - p2 * math.log(p2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(2/3, 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3219280948873623"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(3/5, 2/5) - (4/5) * entropy(3/4, 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = entropy(3/5, 2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01997309402197489"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gain(S, Влажност)\n",
    "hs - (2/5) * entropy(1/2, 1/2) - (3/5) * entropy(2/3, 1/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
